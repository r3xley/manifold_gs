{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gaussian Splatting - Educational Implementation\n",
    "\n",
    "This notebook implements a simplified version of 3D Gaussian Splatting suitable for learning and class projects.\n",
    "\n",
    "**Key Features:**\n",
    "- Axis-aligned Gaussians (no rotation - simplest case)\n",
    "- Vectorized rendering\n",
    "- Fast training (minutes, not hours)\n",
    "- Easy to understand and modify\n",
    "- Ready for diffusion extension\n",
    "\n",
    "**Simplifications:**\n",
    "- No quaternion rotations (axis-aligned only)\n",
    "- Simple diagonal covariance matrices\n",
    "- Reduced number of Gaussians (1000-2000)\n",
    "- Lower resolution (64x64 to 128x128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.insert(0, os.path.abspath('.'))\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Simple Gaussian Model\n",
    "\n",
    "We use **axis-aligned Gaussians** (no rotation) for simplicity:\n",
    "- Position: 3D coordinates (x, y, z)\n",
    "- Scale: 3D scales (sx, sy, sz) - diagonal covariance\n",
    "- Opacity: α ∈ [0, 1]\n",
    "- Color: RGB values\n",
    "\n",
    "Covariance matrix: Σ = diag(sx², sy², sz²)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleGaussian3D(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple 3D Gaussian model with axis-aligned ellipsoids (no rotation).\n",
    "    \n",
    "    Parameters:\n",
    "    - pos: (N, 3) 3D positions\n",
    "    - log_scales: (N, 3) log scales (for positive constraint)\n",
    "    - logit_opacity: (N,) opacity in logit space\n",
    "    - rgb_logit: (N, 3) RGB colors in logit space\n",
    "    \"\"\"\n",
    "    def __init__(self, n_gaussians=1000, device='cpu'):\n",
    "        super().__init__()\n",
    "        self.num = n_gaussians\n",
    "        self.device = device\n",
    "        \n",
    "        # Initialize parameters\n",
    "        # Positions: random near origin\n",
    "        self.pos = nn.Parameter(torch.randn(n_gaussians, 3, device=device) * 0.5)\n",
    "        \n",
    "        # Scales: small initial values (exp(-2) ≈ 0.14)\n",
    "        self.log_scales = nn.Parameter(torch.ones(n_gaussians, 3, device=device) * -2.0)\n",
    "        \n",
    "        # Opacity: high initial value (sigmoid(1) ≈ 0.73)\n",
    "        self.logit_opacity = nn.Parameter(torch.ones(n_gaussians, device=device) * 1.0)\n",
    "        \n",
    "        # Colors: random bright colors\n",
    "        self.rgb_logit = nn.Parameter(torch.randn(n_gaussians, 3, device=device) * 0.5 + 1.0)\n",
    "    \n",
    "    def get_scales(self):\n",
    "        \"\"\"Get positive scales\"\"\"\n",
    "        return torch.exp(self.log_scales)\n",
    "    \n",
    "    def get_opacity(self):\n",
    "        \"\"\"Get opacity in [0, 1]\"\"\"\n",
    "        return torch.sigmoid(self.logit_opacity)\n",
    "    \n",
    "    def get_colors(self):\n",
    "        \"\"\"Get RGB colors in [0, 1]\"\"\"\n",
    "        return torch.sigmoid(self.rgb_logit)\n",
    "    \n",
    "    def get_covariance_3d(self):\n",
    "        \"\"\"\n",
    "        Get 3D covariance matrices (diagonal, axis-aligned).\n",
    "        Returns: (N, 3, 3) covariance matrices\n",
    "        \"\"\"\n",
    "        scales = self.get_scales()  # (N, 3)\n",
    "        # Diagonal covariance: Σ = diag(s²)\n",
    "        cov = torch.zeros(self.num, 3, 3, device=self.device)\n",
    "        cov[:, 0, 0] = scales[:, 0] ** 2\n",
    "        cov[:, 1, 1] = scales[:, 1] ** 2\n",
    "        cov[:, 2, 2] = scales[:, 2] ** 2\n",
    "        return cov\n",
    "\n",
    "# Test the model\n",
    "n_gaussians = 1000\n",
    "gaussians = SimpleGaussian3D(n_gaussians=n_gaussians, device=device)\n",
    "print(f\"✓ Created {n_gaussians} Gaussians\")\n",
    "print(f\"  Position range: [{gaussians.pos.min().item():.2f}, {gaussians.pos.max().item():.2f}]\")\n",
    "print(f\"  Scale range: [{gaussians.get_scales().min().item():.3f}, {gaussians.get_scales().max().item():.3f}]\")\n",
    "print(f\"  Opacity range: [{gaussians.get_opacity().min().item():.3f}, {gaussians.get_opacity().max().item():.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple Data Loader\n",
    "\n",
    "Load LLFF dataset or create synthetic data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_simple_dataset(scene_root=\"data/nerf_llff_data/fern\", downscale=8.0, max_views=None):\n",
    "    \"\"\"\n",
    "    Simple dataset loader for LLFF format.\n",
    "    Returns list of (image, pose, K) tuples.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from data.load_lff import LLFFDataset\n",
    "        dataset = LLFFDataset(scene_root=scene_root, downscale=downscale, device=device)\n",
    "        \n",
    "        # Convert to simple format\n",
    "        data = []\n",
    "        n_views = len(dataset) if max_views is None else min(len(dataset), max_views)\n",
    "        for i in range(n_views):\n",
    "            sample = dataset[i]\n",
    "            data.append({\n",
    "                'image': sample['image'],  # (3, H, W)\n",
    "                'pose': sample['pose'],    # (4, 4)\n",
    "                'K': sample['K'],          # (3, 3)\n",
    "                'H': sample['image'].shape[1],\n",
    "                'W': sample['image'].shape[2]\n",
    "            })\n",
    "        \n",
    "        print(f\"✓ Loaded {len(data)} views from {scene_root}\")\n",
    "        print(f\"  Resolution: {data[0]['H']}x{data[0]['W']}\")\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"⚠ Could not load dataset: {e}\")\n",
    "        print(\"  Creating synthetic data for testing...\")\n",
    "        return create_synthetic_data(n_views=5, H=64, W=64)\n",
    "\n",
    "def create_synthetic_data(n_views=5, H=64, W=64):\n",
    "    \"\"\"Create simple synthetic data for testing\"\"\"\n",
    "    data = []\n",
    "    focal = H * 0.7  # Simple focal length\n",
    "    \n",
    "    for i in range(n_views):\n",
    "        # Simple circular camera path\n",
    "        angle = 2 * np.pi * i / n_views\n",
    "        cam_pos = np.array([np.cos(angle), 0, np.sin(angle)]) * 3.0\n",
    "        \n",
    "        # Simple pose (camera-to-world)\n",
    "        pose = np.eye(4)\n",
    "        pose[:3, 3] = cam_pos\n",
    "        # Look at origin\n",
    "        forward = -cam_pos / np.linalg.norm(cam_pos)\n",
    "        right = np.cross([0, 1, 0], forward)\n",
    "        right = right / np.linalg.norm(right)\n",
    "        up = np.cross(forward, right)\n",
    "        pose[:3, :3] = np.column_stack([right, up, -forward])\n",
    "        \n",
    "        # Simple intrinsics\n",
    "        K = np.array([\n",
    "            [focal, 0, W/2],\n",
    "            [0, focal, H/2],\n",
    "            [0, 0, 1]\n",
    "        ])\n",
    "        \n",
    "        # Create simple test image (checkerboard pattern)\n",
    "        img = np.zeros((3, H, W))\n",
    "        checker = ((np.arange(H)[:, None] // 8) + (np.arange(W)[None, :] // 8)) % 2\n",
    "        img[0] = checker * 0.8\n",
    "        img[1] = (1 - checker) * 0.6\n",
    "        img[2] = checker * 0.4\n",
    "        \n",
    "        data.append({\n",
    "            'image': torch.from_numpy(img).float(),\n",
    "            'pose': torch.from_numpy(pose).float(),\n",
    "            'K': torch.from_numpy(K).float(),\n",
    "            'H': H,\n",
    "            'W': W\n",
    "        })\n",
    "    \n",
    "    print(f\"✓ Created {len(data)} synthetic views\")\n",
    "    return data\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_simple_dataset(scene_root=\"data/nerf_llff_data/fern\", downscale=8.0, max_views=10)\n",
    "print(f\"Dataset size: {len(dataset)} views\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Simple Renderer\n",
    "\n",
    "Project 3D Gaussians to 2D image plane and render using alpha blending.\n",
    "\n",
    "**Steps:**\n",
    "1. Transform 3D positions to camera space\n",
    "2. Project to 2D image coordinates\n",
    "3. Project 3D covariance to 2D\n",
    "4. Evaluate Gaussian at each pixel (vectorized)\n",
    "5. Alpha blend front-to-back"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_render(gaussians, camera_dict):\n",
    "    \"\"\"\n",
    "    Simple renderer for axis-aligned 3D Gaussians.\n",
    "    \n",
    "    Args:\n",
    "        gaussians: SimpleGaussian3D instance\n",
    "        camera_dict: dict with 'pose' (4x4), 'K' (3x3), 'width', 'height'\n",
    "    \n",
    "    Returns:\n",
    "        rendered: (H, W, 3) RGB image\n",
    "    \"\"\"\n",
    "    device = gaussians.device\n",
    "    H, W = int(camera_dict['height']), int(camera_dict['width'])\n",
    "    \n",
    "    # Get parameters\n",
    "    positions_3d = gaussians.pos  # (N, 3)\n",
    "    covariances_3d = gaussians.get_covariance_3d()  # (N, 3, 3) - diagonal\n",
    "    opacities = gaussians.get_opacity()  # (N,)\n",
    "    colors = gaussians.get_colors()  # (N, 3)\n",
    "    \n",
    "    # Transform to camera space\n",
    "    pose = camera_dict['pose'].to(device)  # camera-to-world\n",
    "    w2c = torch.inverse(pose)  # world-to-camera\n",
    "    \n",
    "    # Transform positions: p_cam = R @ p_world + t\n",
    "    positions_cam = (w2c[:3, :3] @ positions_3d.T + w2c[:3, 3:4]).T  # (N, 3)\n",
    "    \n",
    "    # Visibility culling (camera looks along -Z)\n",
    "    visible = positions_cam[:, 2] < -0.01\n",
    "    if visible.sum() == 0:\n",
    "        # Fallback: try positive Z\n",
    "        visible = positions_cam[:, 2] > 0.01\n",
    "        if visible.sum() == 0:\n",
    "            # Use all if still none\n",
    "            visible = torch.ones(len(positions_cam), dtype=torch.bool, device=device)\n",
    "    \n",
    "    positions_cam = positions_cam[visible]\n",
    "    covariances_3d = covariances_3d[visible]\n",
    "    opacities = opacities[visible]\n",
    "    colors = colors[visible]\n",
    "    \n",
    "    # Project to 2D\n",
    "    K = camera_dict['K'].to(device)\n",
    "    fx, fy = K[0, 0], K[1, 1]\n",
    "    cx, cy = K[0, 2], K[1, 2]\n",
    "    \n",
    "    depths = torch.abs(positions_cam[:, 2]).clamp(min=0.01)\n",
    "    x_2d = (positions_cam[:, 0] / depths) * fx + cx\n",
    "    y_2d = (positions_cam[:, 1] / depths) * fy + cy\n",
    "    positions_2d = torch.stack([x_2d, y_2d], dim=-1)  # (N, 2)\n",
    "    \n",
    "    # Project 3D covariance to 2D (simplified for axis-aligned)\n",
    "    # For axis-aligned: project diagonal elements\n",
    "    scales_3d = torch.sqrt(torch.diagonal(covariances_3d, dim1=-2, dim2=-1))  # (N, 3)\n",
    "    \n",
    "    # Simple 2D projection: scale by focal/depth\n",
    "    scales_2d = torch.zeros(len(positions_2d), 2, device=device)\n",
    "    scales_2d[:, 0] = (scales_3d[:, 0] / depths) * fx  # X scale\n",
    "    scales_2d[:, 1] = (scales_3d[:, 1] / depths) * fy  # Y scale\n",
    "    \n",
    "    # Create 2D covariance (diagonal)\n",
    "    covariances_2d = torch.zeros(len(positions_2d), 2, 2, device=device)\n",
    "    covariances_2d[:, 0, 0] = scales_2d[:, 0] ** 2\n",
    "    covariances_2d[:, 1, 1] = scales_2d[:, 1] ** 2\n",
    "    covariances_2d = covariances_2d + 1e-4 * torch.eye(2, device=device).unsqueeze(0)  # Regularization\n",
    "    \n",
    "    # Sort by depth (front-to-back for alpha blending)\n",
    "    depth_order = torch.argsort(depths)\n",
    "    positions_2d = positions_2d[depth_order]\n",
    "    covariances_2d = covariances_2d[depth_order]\n",
    "    opacities = opacities[depth_order]\n",
    "    colors = colors[depth_order]\n",
    "    \n",
    "    # Create pixel grid\n",
    "    y_coords, x_coords = torch.meshgrid(\n",
    "        torch.arange(H, device=device, dtype=torch.float32),\n",
    "        torch.arange(W, device=device, dtype=torch.float32),\n",
    "        indexing='ij'\n",
    "    )\n",
    "    pixels = torch.stack([x_coords.flatten(), y_coords.flatten()], dim=-1)  # (H*W, 2)\n",
    "    \n",
    "    # Render with alpha blending\n",
    "    img = torch.zeros(H * W, 3, device=device)\n",
    "    alpha = torch.ones(H * W, device=device)\n",
    "    \n",
    "    # Process in chunks to save memory\n",
    "    chunk_size = 50\n",
    "    for i in range(0, len(positions_2d), chunk_size):\n",
    "        chunk_end = min(i + chunk_size, len(positions_2d))\n",
    "        pos_chunk = positions_2d[i:chunk_end]\n",
    "        cov_chunk = covariances_2d[i:chunk_end]\n",
    "        opac_chunk = opacities[i:chunk_end]\n",
    "        color_chunk = colors[i:chunk_end]\n",
    "        \n",
    "        # Vectorized evaluation for chunk\n",
    "        for j in range(len(pos_chunk)):\n",
    "            px, py = pos_chunk[j]\n",
    "            \n",
    "            # Bounds check\n",
    "            if px < -50 or px > W + 50 or py < -50 or py > H + 50:\n",
    "                continue\n",
    "            \n",
    "            # Compute Mahalanobis distance (vectorized)\n",
    "            diff = pixels - pos_chunk[j]  # (H*W, 2)\n",
    "            cov_inv = torch.inverse(cov_chunk[j] + 1e-6 * torch.eye(2, device=device))\n",
    "            mahal = torch.sum(diff @ cov_inv * diff, dim=-1)  # (H*W,)\n",
    "            \n",
    "            # Gaussian kernel with 3-sigma cutoff\n",
    "            mask = mahal < 9.0\n",
    "            g = torch.zeros_like(mahal)\n",
    "            g[mask] = torch.exp(-0.5 * mahal[mask])\n",
    "            \n",
    "            # Alpha blending\n",
    "            contribution = alpha.unsqueeze(-1) * opac_chunk[j] * g.unsqueeze(-1) * color_chunk[j].unsqueeze(0)\n",
    "            img = img + contribution\n",
    "            \n",
    "            # Update alpha\n",
    "            alpha = alpha * (1.0 - opac_chunk[j] * g).clamp(0.0, 1.0)\n",
    "    \n",
    "    return torch.clamp(img.reshape(H, W, 3), 0, 1)\n",
    "\n",
    "# Test render\n",
    "test_camera = dataset[0]\n",
    "rendered = simple_render(gaussians, {\n",
    "    'pose': test_camera['pose'],\n",
    "    'K': test_camera['K'],\n",
    "    'width': test_camera['W'],\n",
    "    'height': test_camera['H']\n",
    "})\n",
    "print(f\"✓ Render test successful! Shape: {rendered.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training Loop\n",
    "\n",
    "Simple training with L2 loss and Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_simple(gaussians, dataset, num_epochs=20, lr=0.01, print_every=5):\n",
    "    \"\"\"\n",
    "    Simple training loop.\n",
    "    \n",
    "    Args:\n",
    "        gaussians: SimpleGaussian3D instance\n",
    "        dataset: List of camera dicts\n",
    "        num_epochs: Number of training epochs\n",
    "        lr: Learning rate\n",
    "        print_every: Print progress every N epochs\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(gaussians.parameters(), lr=lr)\n",
    "    \n",
    "    history = {'loss': [], 'psnr': []}\n",
    "    \n",
    "    print(f\"Training for {num_epochs} epochs...\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        num_views = 0\n",
    "        \n",
    "        # Shuffle dataset\n",
    "        indices = torch.randperm(len(dataset))\n",
    "        \n",
    "        for idx in indices:\n",
    "            sample = dataset[int(idx)]\n",
    "            \n",
    "            # Render\n",
    "            camera = {\n",
    "                'pose': sample['pose'],\n",
    "                'K': sample['K'],\n",
    "                'width': sample['W'],\n",
    "                'height': sample['H']\n",
    "            }\n",
    "            img_pred = simple_render(gaussians, camera)  # (H, W, 3)\n",
    "            \n",
    "            # Ground truth\n",
    "            img_gt = sample['image'].permute(1, 2, 0).to(device)  # (H, W, 3)\n",
    "            \n",
    "            # Loss\n",
    "            loss = ((img_pred - img_gt) ** 2).mean()\n",
    "            \n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(gaussians.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            num_views += 1\n",
    "        \n",
    "        avg_loss = epoch_loss / num_views\n",
    "        \n",
    "        # Compute PSNR on first view\n",
    "        if epoch % print_every == 0 or epoch == num_epochs - 1:\n",
    "            with torch.no_grad():\n",
    "                test_sample = dataset[0]\n",
    "                test_camera = {\n",
    "                    'pose': test_sample['pose'],\n",
    "                    'K': test_sample['K'],\n",
    "                    'width': test_sample['W'],\n",
    "                    'height': test_sample['H']\n",
    "                }\n",
    "                rendered = simple_render(gaussians, test_camera)\n",
    "                gt = test_sample['image'].permute(1, 2, 0).to(device)\n",
    "                mse = ((rendered - gt) ** 2).mean()\n",
    "                psnr = -10 * torch.log10(mse + 1e-10)\n",
    "                \n",
    "                history['loss'].append(avg_loss)\n",
    "                history['psnr'].append(psnr.item())\n",
    "                \n",
    "                print(f\"Epoch {epoch:3d}/{num_epochs}: Loss={avg_loss:.5f}, PSNR={psnr:.2f} dB\")\n",
    "    \n",
    "    return history\n",
    "\n",
    "# Initialize Gaussians\n",
    "n_gaussians = 1000\n",
    "gaussians = SimpleGaussian3D(n_gaussians=n_gaussians, device=device)\n",
    "\n",
    "# Train\n",
    "history = train_simple(gaussians, dataset, num_epochs=20, lr=0.01, print_every=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization\n",
    "\n",
    "Visualize results: rendered images, training curves, and 3D Gaussian positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rendered images\n",
    "fig, axes = plt.subplots(2, min(3, len(dataset)), figsize=(15, 10))\n",
    "if len(dataset) == 1:\n",
    "    axes = axes.reshape(2, 1)\n",
    "\n",
    "for i in range(min(3, len(dataset))):\n",
    "    sample = dataset[i]\n",
    "    camera = {\n",
    "        'pose': sample['pose'],\n",
    "        'K': sample['K'],\n",
    "        'width': sample['W'],\n",
    "        'height': sample['H']\n",
    "    }\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        rendered = simple_render(gaussians, camera)\n",
    "    \n",
    "    # Ground truth\n",
    "    gt = sample['image'].permute(1, 2, 0).cpu().numpy()\n",
    "    axes[0, i].imshow(np.clip(gt, 0, 1))\n",
    "    axes[0, i].set_title(f'Ground Truth (View {i})')\n",
    "    axes[0, i].axis('off')\n",
    "    \n",
    "    # Rendered\n",
    "    axes[1, i].imshow(np.clip(rendered.cpu().numpy(), 0, 1))\n",
    "    axes[1, i].set_title(f'Rendered (View {i})')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "epochs = range(0, len(history['loss']) * 5, 5)\n",
    "ax1.plot(epochs, history['loss'])\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True)\n",
    "\n",
    "ax2.plot(epochs, history['psnr'])\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('PSNR (dB)')\n",
    "ax2.set_title('PSNR')\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize 3D Gaussian positions\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "with torch.no_grad():\n",
    "    positions = gaussians.pos.cpu().numpy()\n",
    "    colors = gaussians.get_colors().cpu().numpy()\n",
    "    scales = gaussians.get_scales().cpu().numpy()\n",
    "    opacity = gaussians.get_opacity().cpu().numpy()\n",
    "\n",
    "# Plot Gaussians\n",
    "ax.scatter(positions[:, 0], positions[:, 1], positions[:, 2], \n",
    "           c=colors, s=scales.mean(axis=1)*100, alpha=opacity*0.6)\n",
    "\n",
    "# Plot camera positions if available\n",
    "if len(dataset) > 0:\n",
    "    cam_positions = []\n",
    "    for sample in dataset[:5]:  # First 5 cameras\n",
    "        pose = sample['pose']\n",
    "        if isinstance(pose, torch.Tensor):\n",
    "            pose = pose.cpu().numpy()\n",
    "        cam_pos = pose[:3, 3]\n",
    "        cam_positions.append(cam_pos)\n",
    "    \n",
    "    if cam_positions:\n",
    "        cam_positions = np.array(cam_positions)\n",
    "        ax.scatter(cam_positions[:, 0], cam_positions[:, 1], cam_positions[:, 2],\n",
    "                  c='red', s=50, marker='^', label='Cameras')\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "ax.set_title('3D Gaussian Positions')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Extension Ideas (for Diffusion)\n",
    "\n",
    "This simplified implementation can be extended with diffusion:\n",
    "\n",
    "1. **Parameter Space**: The Gaussian parameters (pos, scales, colors, opacity) form a parameter space\n",
    "2. **Diffusion Process**: Add noise and denoise in this space\n",
    "3. **Manifold Structure**: Consider the manifold structure of rotations (if adding rotations later)\n",
    "\n",
    "**Simple diffusion extension:**\n",
    "- Add noise to Gaussian parameters: `θ_t = θ_0 + ε * noise`\n",
    "- Train a denoising network or use iterative denoising\n",
    "- Sample new Gaussians via reverse diffusion process"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
